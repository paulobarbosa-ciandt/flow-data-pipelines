{"dashboard":{"name":"[APAC Data Demo] E2E Delta Lake Deployment Acceleration","description":"End-to-end acceleration of Delta Lake deployments, from business requirements through sourcing and transforming data to quality and BI.","category":"507f1f77bcf86cd799439013","isActive":true,"columns":[{"name":"Business Requirements","description":"Business Requirements document to be parsed and converted into stories.","templates":["0"],"extensions":[]},{"name":"User Stories","description":"Stories broken down from requirements","templates":["1","2"],"extensions":[]},{"name":"Code","description":"Code to implement the data pipeline","templates":[],"extensions":[]}],"rules":[{"emails":[],"groups":[],"roles":["backlogrefiner.admin"]}]},"templates":[{"author":"4a8877b1fbb94571bb11d575","name":"data__business_requirements_to_stories","domain":"put_here_your_domain","category":"507f1f77bcf86cd799439011","context":"You are a Data Architect skilled in translating business requirements into clear, actionable agile user stories. Your task is to generate user stories based **solely** on the provided requirements document, adhering strictly to the given templates and instructions. Integrate all business logic and data quality checks within the technical stories. Use the following technology stack: **Azure** (cloud platform), Databricks (data processing and Delta Lake storage), and dbt (data transformation and data quality validations).\n","prompt":"A requirements document has been attached for implementing a data pipeline on our data platform.\n\n---\n\n#### **Your Tasks**\n\n1. **Extract All Relevant Details and List Them Explicitly:**\n\n   - **Table Information:**\n     - **Table Names:** List all table names.\n     - **Constructed Table Names:**\n       - **Raw Table Name:** `raw_{{table_name}}` (for tables in `fivetran` schema)\n       - **Bronze Table Name:** `bronze_{{table_name}}`\n       - **Silver Table Name:** `silver_{{table_name}}`\n     - **Schemas:**\n       - **Source Schema:** `fivetran`\n       - **Bronze Schema:** `bronze`\n       - **Silver Schema:** `silver`\n     - **Full Table Names with Schemas:**\n       - **Source Table:** `{{source_schema}}.raw_{{table_name}}`\n       - **Bronze Table:** `{{bronze_schema}}.{{bronze_schema}}_{{table_name}}`\n       - **Silver Table:** `{{silver_schema}}.{{silver_schema}}_{{table_name}}`\n     - **Source Details:** Provide details like database name, table name, file path.\n     - **List of Fields for Each Table:** List all fields explicitly.\n   - **Business Logic Details:**\n     - **Transformations:** List all transformations to be applied.\n     - **Calculations:** List all calculations explicitly.\n     - **Mappings:**\n       - **Mapping Tables:** Provide mapping details.\n   - **Data Quality Requirements:**\n     - **Types of Checks:** List all data quality checks required.\n   - **Dimensional Modeling Details:**\n     - **List of Fact and Dimension Tables:** Provide names of all tables.\n     - **Slowly Changing Dimensions (SCD) Requirements**\n     - **Metadata Columns to be Added**\n     - **Surrogate Key Requirements**\n\n2. **Generate User Stories:**\n\n   - **Generate a separate user story for each table in all layers (bronze, silver, and gold).**\n   - Use **only** the extracted details to fill in all placeholders in the templates below.\n   - Ensure each user story is complete with all necessary information replaced.\n   - **Include data quality checks within the transformation user stories. Do not create separate user stories for data quality checks.**\n   - **Use the constructed table names with schemas as appropriate.**\n   - **DO NOT ADD ANY FIELDS, CALCULATIONS, TRANSFORMATIONS, MAPPINGS, OR ASSUMPTIONS THAT ARE NOT EXPLICITLY PROVIDED IN THE REQUIREMENTS DOCUMENT.**\n\n3. **Validate Against Requirements:**\n\n   - **Before finalizing each user story, cross-check that all fields, calculations, transformations, mappings, and data used are present in the requirements document. DO NOT INCLUDE ANY ADDITIONAL INFORMATION.**\n\n4. **Insert Separators:**\n\n   - Place the separator `{{wf.template.splitBy}}` **only between** each user story.\n   - **Do not** include the separator before the first story or after the last one.\n   - **Do not** mention the separator within the user stories themselves.\n\n5. **Output Format:**\n\n   - **Only** include the content of each user story followed by the separator.\n   - **Exclude** any titles, comments, explanations, or additional text outside the user stories.\n   - **Do not** include any introductory text, explanations, or headings in the output.\n\n---\n\n#### **List of Required User Stories**\n\n##### **1. Ingest Source Data into Bronze Layer**\n\n*Generate a separate user story for each source table.*\n\n```markdown\n- **As a** data engineer\n- **I want** to ingest `{{table_name}}` from the `{{source_schema}}` schema into the bronze layer in Databricks on Azure\n- **So that** it is available for downstream processing\n\n- **Acceptance Criteria**\n\n  - Data is ingested from `{{source_schema}}.raw_{{table_name}}` to `{{bronze_schema}}.{{bronze_schema}}_{{table_name}}` in Delta Lake format\n  - Includes all relevant fields: `{{list_of_fields}}`\n  - Handles data ingestion failures gracefully\n\n- **Technical Details**\n\n  - **Source Table**: `{{source_schema}}.raw_{{table_name}}`\n  - **Target Table**: `{{bronze_schema}}.{{bronze_schema}}_{{table_name}}`\n  - **Source Details**: `{{source_details}}`\n  - **Destination Table Schema**\n\n    ```\n    | Column Name   | Data Type     | Constraints        |\n    |---------------|---------------|--------------------|\n    {{destination_table_schema}}\n    ```\n\n  - **Include Metadata Columns**\n    - Add `create_date` and `update_date` columns.\n  - **Ingestion Method**: `{{ingestion_method}}`\n  - **Tools/Technologies**: Databricks on Azure, Delta Lake, Azure Blob Storage for storage\n```\n\n##### **2. Transform Data from Bronze to Silver Layer with Business Logic and Data Quality Checks**\n\n*Generate a separate user story for each bronze table.*\n\n```markdown\n- **As a** data engineer\n- **I want** to transform and cleanse `{{table_name}}` from the bronze layer to the silver layer using dbt in Databricks on Azure, applying necessary business logic and data quality checks\n- **So that** the data is accurate and ready for analytical processing\n\n- **Acceptance Criteria**\n\n  - Apply transformations to standardize data fields according to the data model\n  - Integrate business logic calculations as specified\n  - **Implement data quality checks within the dbt models**\n    - Checks cover null values, data type mismatches, and business rule validations\n  - Ensure data passes all quality checks before loading into `{{silver_schema}}.{{silver_schema}}_{{table_name}}`\n\n- **Technical Details**\n\n  - **Source Table**: `{{bronze_schema}}.{{bronze_schema}}_{{table_name}}`\n  - **Target Table**: `{{silver_schema}}.{{silver_schema}}_{{table_name}}`\n  - **Transformations and Business Logic**\n    - **Calculations:** `{{calculations}}`\n    - **Transformations:** `{{transformations}}`\n    - **Mappings:** `{{mappings}}`\n      ```\n      {{mapping_table}}\n      ```\n  - **Data Quality Checks**\n    - **Null Checks:** Ensure critical fields are not null (`{{critical_fields}}`)\n    - **Data Type Validations:** Confirm data types match the schema\n    - **Business Rule Validations:** Validate data according to business rules (`{{business_rules}}`)\n  - **Data Types and Constraints**\n\n    ```\n    | Column Name   | Data Type     | Constraints        |\n    |---------------|---------------|--------------------|\n    {{data_types_and_constraints}}\n    ```\n\n  - **Include Metadata Columns**\n    - Add `create_date` and `update_date` columns.\n  - **Tools/Technologies**: dbt, Databricks on Azure, Delta Lake\n```\n\n##### **3. Design and Create Dimensional Model in Gold Layer**\n\n*Generate a separate user story for each table (fact and dimension) in the gold layer.*\n\n```markdown\n- **As a** data engineer\n- **I want** to design and implement the `{{table_name}}` table in the gold layer using Databricks and Delta Lake on Azure, considering necessary dimensional modeling techniques, using snake_case for data objects and column names, and adding relevant metadata columns\n- **So that** end-users can perform efficient analytics with accurate historical data\n\n- **Acceptance Criteria**\n\n  - The `{{table_name}}` table is created according to the dimensional model requirements\n  - Implement **SCD Type 2** for dimension tables (if applicable), including metadata columns:\n    - `effective_date`: Start date of the record's validity\n    - `end_date`: End date of the record's validity (use high date `'9999-12-31'` for current records)\n    - `is_current`: Boolean flag indicating if the record is the most current one\n  - Use **snake_case** for all table names and column names\n  - Add relevant metadata columns to the table, such as `create_date`, `update_date`, and suggest other relevant columns according to best practices\n  - Include surrogate key columns in dimension tables\n  - The table is optimized for query performance\n  - Data lineage is documented\n\n- **Technical Details**\n\n  - **Table Definition**\n\n    ```\n    | Column Name   | Data Type     | Constraints        |\n    |---------------|---------------|--------------------|\n    {{table_definition}}\n    ```\n\n  - **Keys and Indexes**\n\n    - **Primary Key**: `{{primary_key}}`\n    - **Foreign Keys**: `{{foreign_keys}}` (if any)\n\n  - **SCD Implementation** (for dimension tables):\n\n    - Implement SCD Type 2 logic as per requirements\n\n  - **Tools/Technologies**: Databricks on Azure, Delta Lake\n```\n\n---\n\n#### **Important Instructions**\n\n- **USE ONLY PROVIDED INFORMATION:**\n\n  - **ONLY USE DETAILS EXPLICITLY PROVIDED IN THE REQUIREMENTS DOCUMENT. DO NOT ADD ANY FIELDS, CALCULATIONS, TRANSFORMATIONS, MAPPINGS, OR ASSUMPTIONS THAT ARE NOT SPECIFIED.**\n\n- **Tables Naming Convention:**\n\n  - **Source Schema:** `fivetran`\n  - **Bronze Schema:** `bronze`\n  - **Silver Schema:** `silver`\n  - **Gold Schema (for dimensional model):** `gold`\n  - **Table Naming:**\n    - **Source Tables in `fivetran` Schema:** Prefixed with `raw_`, e.g., `fivetran.raw_members`\n    - **Bronze Tables:** `<schema>.<schema_name>_<table_name>`, e.g., `bronze.bronze_members`\n    - **Silver Tables:** `<schema>.<schema_name>_<table_name>`, e.g., `silver.silver_members`\n    - **Gold Tables:** `<schema>.<table_name>`, e.g., `gold.dim_members`, `gold.fact_transactions`\n\n- **Generate Separate User Stories for Each Table:**\n\n  - **For the bronze, silver, and gold layers, create a separate user story for each table.**\n\n- **Integration of Data Quality Checks:**\n\n  - **Include data quality checks within the transformation user stories. Do not create separate user stories for data quality checks.**\n  - **Data quality checks should be implemented within the dbt models as part of the transformation process.**\n\n- **Extraction Phase:**\n\n  - **Carefully read** the attached requirements document.\n  - **Extract and list** all necessary details as outlined in the \"Extract All Relevant Details and List Them Explicitly\" section.\n  - **Construct** table names with the appropriate prefixes and schemas.\n\n- **Generation Phase:**\n\n  - **Use only the extracted and listed details** to replace all placeholders within each user story template.\n  - **Ensure** that every placeholder is correctly replaced.\n  - **DO NOT ADD ANY EXTRA INFORMATION OR MAKE ASSUMPTIONS BEYOND THE PROVIDED DETAILS.**\n\n- **Validation Step:**\n\n  - **Before finalizing each user story, cross-check that all fields, calculations, transformations, mappings, and data used are present in the requirements document. DO NOT INCLUDE ANY ADDITIONAL INFORMATION.**\n\n- **Separator Placement:**\n\n  - **Insert** the separator `{{wf.template.splitBy}}` **only between** each user story.\n  - **Do not** place the separator before the first story or after the last one.\n  - **Ensure** that the separator is **not** mentioned within the user stories themselves.\n\n- **Output Format:**\n\n  - **Only** include the content of each user story followed by the separator.\n  - **Exclude** any titles, comments, explanations, or additional text outside the user stories.\n  - **Do not** include any introductory text, explanations, or headings in the output.\n\n- **Language and Formatting:**\n\n  - **All output** should be in English.\n  - **Follow** the specified format in the templates precisely.\n  - **Adhere** to the acceptance criteria and technical details sections, providing comprehensive information for developers.\n\n- **Final Check:**\n\n  - **Ensure** that the final output contains **only** the user stories with correctly prefixed table names and the separators, with **no** additional content.\n  - **CONFIRM** that no additional fields, calculations, transformations, mappings, or assumptions have been included.\n\n---\n\n**Separator Between Stories**\n\n```\n{{wf.template.splitBy}}\n```","example":"<Story Title>\nAS A <type of user>\nI WANT <brief description of the story>\nSO THAT <user's objective in up to 15 words>\n\nAcceptance Criteria\n<acceptance criteria>\n\nTechnical Details\n<technical details>","splitBy":"<<<->>>","enabled":true,"completionOptions":{"model":"gpt-4o","temperature":0.1}},{"author":"4a8877b1fbb94571bb11d575","name":"data__load_bronze","domain":"put_here_your_domain","category":"507f1f77bcf86cd799439013","context":"You are ChatGPT, a large language model trained by OpenAI with expertise in data engineering, dbt, Delta Lake, Python, and Azure Blob Storage within Databricks environments. Your role is to generate precise `dbt` model and schema files for ingesting Parquet data into the bronze layer of a data lake. Follow best practices, use **snake_case** for all identifiers, and ensure consistency with the provided source configurations.","prompt":"{{wf.parent.content}}\n\n---\n\n### **Data Source Configuration**\n\n- **Azure Blob Storage Details:**\n  - **Storage Account**: `flowdataefficiencydemo`\n  - **Container Name**: `fivetran`\n  - **File Format**: Parquet\n  - **File Location Pattern**: Data for each source table is organized into separate directories within the `fivetran` container following the naming pattern `raw_<source_table_name>`.\n    - *Example*: Data from the source table `members` is located in `fivetran/raw_members`.\n  - **Destination Table**: `bronze.<source_table_name>`\n\n---\n\n### **Requirements**\n\n#### **1. Data Ingestion**\n\n- **Assumption**: Fivetran is already configured to extract data from the SQL Server database and load it into the Azure Blob Storage container named `fivetran` under the account `flowdataefficiencydemo`.\n\n#### **2. dbt Model and Schema File Generation**\n\n- **Model File Naming:**\n  - **Naming Convention**: The `dbt` model file should be named **exactly** as the table name with a `.sql` extension.\n    - *Example*: For the table `members`, the model file should be `members.sql`.\n\n- **Schema File Generation:**\n  - **Mandatory Schema Files**: For each `dbt` model, generate a corresponding schema file named `<table_name>.yml` (e.g., `members.yml`).\n  - **Schema Configuration**: The schema file should include:\n    - **Models**: Metadata and descriptions for each column.\n    - **Exclusions**: **Do not** include any constraints, data quality tests, or validation rules.\n\n- **Delta Table Creation:**\n  - Generate `dbt` models to create the Delta table in the bronze layer if it does not exist.\n  - If the Delta table exists and there are schema changes, generate `dbt` models to alter the table accordingly.\n  - **Schema Adherence**: Ensure that the `dbt` models strictly adhere to the source table schemas as defined in the Hive Metastore catalog, maintaining a 1-to-1 mapping without introducing additional columns.\n\n- **Partitioning:**\n  - Suggest appropriate partition keys based on the source schema to optimize performance.\n\n- **Delta Table Settings:**\n  - Include relevant Delta table configurations to enhance performance and reliability.\n\n- **Metadata Columns:**\n  - Add `create_date` and `update_date` columns to the Delta table using appropriate data types (e.g., `TIMESTAMP`).\n\n- **Error Handling:**\n  - Ensure that the `dbt` models handle data ingestion failures gracefully.\n  - Incorporate necessary logging mechanisms.\n\n- **Best Practices:**\n  - Follow best practices for Delta Lake, `dbt`, and Python development.\n  - Use **snake_case** for all data objects and column names.\n  - Avoid hardcoding sensitive information; utilize secure parameter storage where necessary.\n\n- **Specific Adjustments:**\n  - **Incremental Strategy**: Use the `merge` strategy without including aggregation functions in the `WHERE` clause, as Databricks does not handle them efficiently. Do not include an `is_incremental()` block if there are no additional conditions or transformations needed for incremental loads.\n  - **Source Reference**: Ensure that the `{{ source('fivetran', 'raw_<source_table_name>') }}` reference is properly configured.\n\n#### **3. Source Configuration File (`sources.yml`)**\n\n- **Generation/Update Instructions:**\n  - Generate or update a `sources.yml` file within a directory named `sources` in the project root.\n  - The `sources.yml` file should define the source configurations as follows:\n\n```yaml\nversion: 2\n\nsources:\n  - name: fivetran\n    schema: fivetran  \n    tables:\n      - name: raw_<source_table_name>\n```\n\n- **Ensure Consistency**: The source configurations in `sources.yml` should align with the `dbt` models to maintain consistency and integrity.\n\n#### **4. Schema File (`<table_name>.yml`)**\n\n- **Generation/Update Instructions:**\n  - For each `dbt` model, generate or update a corresponding schema file named `<table_name>.yml` (e.g., `members.yml`).\n  - The schema file should include:\n    - **Model Description**: A brief description of the model's purpose.\n    - **Columns**: Definitions for each column, including descriptions and data types.\n    - **Exclusions**: **Do not** include any constraints, data quality tests, or validation rules.\n\n- **Example Schema Configuration:**\n\n```yaml\nversion: 2\n\nmodels:\n  - name: members\n    description: \"Bronze layer table for members data ingested from Azure Blob Storage.\"\n    columns:\n      - name: member_id\n        description: \"Unique identifier for each member.\"\n      - name: first_name\n        description: \"Member's first name.\"\n      - name: last_name\n        description: \"Member's last name.\"\n      - name: date_of_birth\n        description: \"Member's date of birth.\"\n      - name: gender\n        description: \"Member's gender.\"\n      - name: address\n        description: \"Member's address.\"\n      - name: phone_number\n        description: \"Member's phone number.\"\n      - name: email\n        description: \"Member's email address.\"\n      - name: join_date\n        description: \"Date when the member joined.\"\n      - name: employer_id\n        description: \"Identifier for the member's employer.\"\n      - name: modified_date\n        description: \"Timestamp of the last modification.\"\n      - name: create_date\n        description: \"Timestamp when the record was created.\"\n      - name: update_date\n        description: \"Timestamp when the record was last updated.\"\n```\n\n#### **5. Additional Recommendations**\n\n- Suggest any improvements or optimizations for the data ingestion process to enhance efficiency, reliability, or scalability.\n\n---\n\n### **Example Adjustments Based on Existing Issues**\n\n- **Removing Aggregation in WHERE Clause**: Do not include date checks using aggregation functions in the `WHERE` clause. Rely solely on the `merge` strategy for incremental updates.\n  \n- **Excluding Empty `is_incremental()` Blocks**: Omit the `is_incremental()` conditional block if there are no additional conditions or transformations required for incremental loads.\n  \n- **Including `sources.yml` Configuration**: Ensure that the `sources.yml` file is generated or updated to define the source tables referenced in the `dbt` models.\n\n- **Generating Schema Files Without Constraints or Tests**: For each `dbt` model, generate a corresponding schema file (`<table_name>.yml`) that includes only model descriptions and column metadata, excluding any constraints or data quality tests.\n\n---\n\n### **Expected Output**\n\n1. **dbt Model File**: A properly configured `dbt` model for ingesting data into the bronze layer without using aggregation functions in the `WHERE` clause and without unnecessary `is_incremental()` blocks. The model file should be named after the table (e.g., `members.sql`).\n\n2. **Schema File**: A corresponding schema file (`members.yml`) located alongside the model file with accurate model descriptions and column definitions **without** any constraints or data quality tests.\n\n3. **sources.yml File**: A `sources.yml` file located in the `sources` directory with accurate source configurations.\n\n---\n\n### **Sample dbt Model Generated**\n\n```sql\n{{ config(\n    materialized='incremental',\n    unique_key='member_id',\n    incremental_strategy='merge',\n    on_schema_change='sync_all_columns'\n) }}\n\nWITH source_data AS (\n    SELECT\n        member_id,\n        first_name,\n        last_name,\n        date_of_birth,\n        gender,\n        address,\n        phone_number,\n        email,\n        join_date,\n        employer_id,\n        modified_date,\n        current_timestamp() AS create_date,\n        current_timestamp() AS update_date\n    FROM\n        {{ source('fivetran', 'raw_members') }}\n)\n\nSELECT\n    member_id,\n    first_name,\n    last_name,\n    date_of_birth,\n    gender,\n    address,\n    phone_number,\n    email,\n    join_date,\n    employer_id,\n    modified_date,\n    create_date,\n    update_date\nFROM\n    source_data\n```\n\n### **Sample Schema File (`members.yml`)**\n\n```yaml\nversion: 2\n\nmodels:\n  - name: members\n    description: \"Bronze layer table for members data ingested from Azure Blob Storage.\"\n    columns:\n      - name: member_id\n        description: \"Unique identifier for each member.\"\n      - name: first_name\n        description: \"Member's first name.\"\n      - name: last_name\n        description: \"Member's last name.\"\n      - name: date_of_birth\n        description: \"Member's date of birth.\"\n      - name: gender\n        description: \"Member's gender.\"\n      - name: address\n        description: \"Member's address.\"\n      - name: phone_number\n        description: \"Member's phone number.\"\n      - name: email\n        description: \"Member's email address.\"\n      - name: join_date\n        description: \"Date when the member joined.\"\n      - name: employer_id\n        description: \"Identifier for the member's employer.\"\n      - name: modified_date\n        description: \"Timestamp of the last modification.\"\n      - name: create_date\n        description: \"Timestamp when the record was created.\"\n      - name: update_date\n        description: \"Timestamp when the record was last updated.\"\n```\n\n### **Sample sources.yml File**\n\n```yaml\nversion: 2\n\nsources:\n  - name: fivetran\n    schema: fivetran  \n    tables:\n      - name: raw_members\n```","example":"","splitBy":"","enabled":true,"completionOptions":{"model":"gpt-4o","temperature":0.1}},{"author":"4a8877b1fbb94571bb11d575","name":"data__load_silver","domain":"put_here_your_domain","category":"507f1f77bcf86cd799439013","context":"You are ChatGPT, a large language model trained by OpenAI with expertise in data engineering, dbt, Delta Lake, Python, and Azure Databricks environments. Your role is to generate precise dbt model and schema files for transforming and cleansing data from the bronze layer to the silver layer of a data lake, adhering to best practices, the `<layer>_<table_name>` naming convention, and the provided technical specifications.","prompt":"{{wf.parent.content}}\n\n---\n\n**Source Details: Bronze Layer**\n\n- **Primary Source Table**: {{primary_source_table_name}}\n- **Additional Source Tables**: \n  {% if additional_source_tables %}\n    {% for table in additional_source_tables %}\n      - **Table Name**: {{ table.name }}\n        - **Schema**: bronze\n        - **Data Format**: Delta Lake\n        - **Storage Location**: Delta tables in Databricks on Azure\n    {% endfor %}\n  {% endif %}\n\n- **Schema**: bronze\n- **Data Format**: Delta Lake\n- **Storage Location**: Delta tables in Databricks on Azure\n\n**Target Details: Silver Layer**\n\n- **Target Table**: {{target_table_name}}\n- **Schema**: silver\n- **Data Format**: Delta Lake\n- **Storage Location**: Delta tables in Databricks on Azure\n\n---\n\n**Transformations and Business Logic:**\n\n{% if transformations %}\n### Calculations:\n{% for calc in transformations.calculations %}\n- `{{ calc.expression }}` as `{{ calc.alias }}`\n{% endfor %}\n\n### Mappings:\n- **Standardize String Fields to Proper Case**\n  ```\n  | Source Column | Transformation          | Target Column     |\n  |---------------|-------------------------|-------------------|\n  {% for mapping in transformations.mappings %}\n  | {{ mapping.source }} | {{ mapping.transformation }} | {{ mapping.target }} |\n  {% endfor %}\n  ```\n{% endif %}\n\n### Data Quality Checks:\n- **Key Columns Should Not Contain Nulls:** `{{ key_columns | join(', ') }}`\n- **Categorical Fields Should Contain Only Expected Values:** \n  - `{{ categorical_field }} IN ({{ expected_values | join(', ') }})`\n\n---\n\n**Requirements:**\n\n- **dbt Model Generation:**\n  - Create a dbt model to transform data from `bronze.{{primary_source_table_name}}` and any **additional source tables** (e.g., `bronze.{{table.name}}`) to `silver.{{target_table_name}}`.\n  - Ensure transformations adhere strictly to the specified business logic and mappings.\n  - Maintain a 1-to-1 mapping of necessary columns without introducing additional columns unless specified.\n  - Appropriately join additional source tables based on the transformation requirements.\n  - **Adhere to the `<layer>_<table_name>` naming convention for all table references.** Use the provided table names as is.\n\n- **Schema Configuration (`schema.yml`):**\n  - Define the target table schema as specified in the technical details.\n  - Include `create_date` and `update_date` metadata columns with appropriate data types (e.g., TIMESTAMP).\n  - **Integrate Data Quality Checks as dbt Tests:**\n    - Add `not_null`, `unique`, and `accepted_values` tests based on the specified data quality checks.\n  \n- **Incremental Loading:**\n  - Implement incremental models using the `merge` strategy without additional WHERE clauses that filter based on dates.\n  - Rely on the merge strategy to handle data updates efficiently.\n\n- **Source Configuration (`sources.yml`):**\n  - Ensure that the `sources.yml` file includes the `silver` schema and references to `bronze.{{primary_source_table_name}}` as well as any **additional source tables**.\n  - If `sources.yml` does not exist or needs updating, generate or modify it accordingly.\n  - **Maintain the `<layer>_<table_name>` naming convention within the `sources.yml` file.**\n\n- **Target Schema Configuration:**\n  - Generate a `schemas/{{target_table_name}}.yml` file within a directory named `schemas` in the project root.\n  - Define the target table's schema, including columns, data types, and descriptions as specified.\n  - **Do not include any constraints** since Databricks does not enforce them.\n  - **Do not include any quality tests** in the schema files beyond dbt's standard tests.\n  - Ensure all table and column names follow the `<layer>_<table_name>` naming convention.\n\n- **Error Handling:**\n  - Implement graceful error handling within the dbt models.\n  - Include logging mechanisms to capture and report data transformation issues.\n\n- **Best Practices:**\n  - Use **snake_case** for all data objects and column names.\n  - Avoid hardcoding sensitive information; utilize secure parameter storage where necessary.\n  - Follow best practices for Delta Lake and dbt development to ensure performance and maintainability.\n  - **Ensure all table names adhere to the `<layer>_<table_name>` naming convention.** Use the provided table names as is in all `source` function references.\n\n- **Additional Recommendations:**\n  - Suggest optimizations for the transformation process, such as indexing frequently queried columns.\n  - Recommend any additional metadata or documentation that could enhance data quality and usability.\n\n---\n\n**Technical Details:**\n\n- **Tools/Technologies**: dbt, Databricks on Azure, Delta Lake\n- **Programming Language**: SQL with Jinja templating for dbt\n\n---\n\n**Example Generated Files:**\n\n1. **dbt Model (`silver_members.sql`):**\n\n    ```sql\n    {{ config(\n        materialized='incremental',\n        unique_key='member_id',\n        incremental_strategy='merge',\n        on_schema_change='sync_all_columns'\n    ) }}\n\n    WITH source_data AS (\n        SELECT\n            m.member_id,\n            CONCAT(UPPER(SUBSTRING(m.first_name, 1, 1)), LOWER(SUBSTRING(m.first_name, 2))) AS first_name,\n            CONCAT(UPPER(SUBSTRING(m.last_name, 1, 1)), LOWER(SUBSTRING(m.last_name, 2))) AS last_name,\n            m.date_of_birth,\n            m.gender,\n            m.join_date,\n            m.employer_id,\n            SUM(c.amount) - SUM(w.amount) AS account_balance,\n            current_timestamp() AS create_date,\n            current_timestamp() AS update_date\n        FROM\n            {{ source('bronze', '{{primary_source_table_name}}') }} AS m\n            LEFT JOIN {{ source('bronze', 'bronze_contributions') }} AS c ON m.member_id = c.member_id\n            LEFT JOIN {{ source('bronze', 'bronze_withdrawals') }} AS w ON m.member_id = w.member_id\n        GROUP BY\n            m.member_id, m.first_name, m.last_name, m.date_of_birth, m.gender, m.join_date, m.employer_id\n    )\n\n    SELECT\n        member_id,\n        CONCAT(first_name, ' ', last_name) AS full_name,\n        date_of_birth,\n        gender,\n        join_date,\n        employer_id,\n        account_balance,\n        create_date,\n        update_date\n    FROM\n        source_data\n    ```\n\n2. **Sources Configuration (`sources.yml`):**\n\n    ```yaml\n    version: 2\n\n    sources:\n      - name: bronze\n        schema: bronze\n        tables:\n          - name: bronze_members\n          - name: bronze_contributions\n          - name: bronze_withdrawals\n\n      - name: silver\n        schema: silver\n        tables:\n          - name: silver_members\n    ```\n\n3. **Target Table Schema (`schemas/silver_members.yml`):**\n\n    ```yaml\n    version: 2\n\n    models:\n      - name: silver_members\n        description: \"Transformed and cleansed data from bronze.bronze_members, bronze.bronze_contributions, and bronze.bronze_withdrawals for analytical processing.\"\n        columns:\n          - name: member_id\n            description: \"Unique identifier for each member.\"\n            data_type: INT\n            tests:\n              - not_null\n              - unique\n          - name: first_name\n            description: \"Member's first name, standardized to proper case.\"\n            data_type: VARCHAR(50)\n            tests:\n              - not_null\n          - name: last_name\n            description: \"Member's last name, standardized to proper case.\"\n            data_type: VARCHAR(50)\n            tests:\n              - not_null\n          - name: date_of_birth\n            description: \"Member's date of birth.\"\n            data_type: DATE\n            tests:\n              - not_null\n          - name: gender\n            description: \"Member's gender.\"\n            data_type: CHAR(1)\n            tests:\n              - not_null\n              - accepted_values:\n                  values: ['M', 'F', 'O']  # Example accepted values\n          - name: join_date\n            description: \"Date when the member joined.\"\n            data_type: DATE\n            tests:\n              - not_null\n          - name: employer_id\n            description: \"Identifier for the member's employer.\"\n            data_type: INT\n            tests:\n              - not_null\n          - name: account_balance\n            description: \"Calculated account balance for the member.\"\n            data_type: DECIMAL(10,2)\n            tests:\n              - not_null\n          - name: create_date\n            description: \"Timestamp when the record was created.\"\n            data_type: TIMESTAMP\n            tests:\n              - not_null\n          - name: update_date\n            description: \"Timestamp when the record was last updated.\"\n            data_type: TIMESTAMP\n            tests:\n              - not_null\n        meta:\n          created_at: \"2024-04-27T12:00:00Z\"\n          updated_at: \"2024-04-27T12:00:00Z\"\n    ```\n\n---\n\n### **Detailed Explanation of Adjustments:**\n\n1. **Incorporate Layer Prefixes Directly in Table Names:**\n   - **User Input:** By requiring users to provide table names with the layer prefix (e.g., `bronze_members`), we eliminate the need for the prompt to add them automatically, ensuring clarity and consistency.\n   - **Prompt Adjustment:** Removed any logic that attempts to prepend layer prefixes. The prompt now uses the provided table names directly in all references.\n\n2. **Integrate Data Quality Checks as dbt Tests:**\n   - **Schema Configuration (`schema.yml`):** Added a `tests` section under each column in the `schema.yml` file. These tests leverage dbt's built-in testing framework to enforce data quality constraints.\n     - **Example Tests:**\n       - `not_null`: Ensures that the column does not contain null values.\n       - `unique`: Ensures that all values in the column are unique.\n       - `accepted_values`: Ensures that the column contains only specified values (useful for categorical fields).\n\n3. **Consistent Naming Conventions Across All Files:**\n   - **dbt Models, `schema.yml`, and `sources.yml`:** All references to tables now include the layer prefix, maintaining consistency and preventing any naming discrepancies.\n\n4. **Explicit Instructions for Data Quality Checks:**\n   - **Transformations and Business Logic Section:** Clearly defines what data quality checks need to be implemented, which the prompt translates into dbt tests within the `schema.yml` files.\n\n5. **Eliminate Ambiguity in Join Conditions:**\n   - **Join Conditions:** Since table names include layer prefixes, join conditions are straightforward and unambiguous, referencing fully prefixed table names.\n\n---\n\n### **Updated User Prompt Example:**\n\nTo ensure the prompt functions as intended, here's how your **User Prompt** should look with fully prefixed table names:\n\n```markdown\n- **As a** data engineer\n- **I want** to transform and cleanse `bronze_members`, `bronze_contributions`, and `bronze_withdrawals` from the bronze layer to the silver layer using dbt in Databricks on Azure, applying necessary business logic\n- **So that** the data is accurate and ready for analytical processing\n\n- **Acceptance Criteria**\n\n  - Apply transformations to standardize data fields according to the data model\n  - Integrate business logic calculations as specified\n  - Ensure data passes all quality checks\n\n- **Technical Details**\n\n  - **Source Tables**: `bronze_members`, `bronze_contributions`, `bronze_withdrawals`\n  - **Target Table**: `silver_members`\n  - **Transformations and Business Logic**\n    - **Calculations**: `account_balance = SUM(bronze_contributions.amount) - SUM(bronze_withdrawals.amount)`\n    - **Mappings**: `Standardize string fields to proper case`\n      ```\n      | Source Column | Transformation          | Target Column     |\n      |---------------|-------------------------|-------------------|\n      | first_name    | Proper Case             | first_name        |\n      | last_name     | Proper Case             | last_name         |\n      ```\n  - **Data Types**\n    ```\n    | Column Name      | Data Type     |\n    |------------------|---------------|\n    | member_id        | INT           |\n    | full_name        | VARCHAR(100)  |\n    | date_of_birth    | DATE          |\n    | gender           | CHAR(1)       |\n    | join_date        | DATE          |\n    | employer_id      | INT           |\n    | account_balance  | DECIMAL(10,2) |\n    | create_date      | TIMESTAMP     |\n    | update_date      | TIMESTAMP     |\n    ```\n\n  - **Include Metadata Columns**\n    - Add `create_date` and `update_date` columns.\n  - **Ingestion Method**: Incremental load using `merge` strategy\n  - **Tools/Technologies**: dbt, Databricks on Azure, Delta Lake\n\n  - **Join Conditions:**\n    - `bronze_members.member_id = bronze_contributions.member_id`\n    - `bronze_members.member_id = bronze_withdrawals.member_id`","example":"","splitBy":"","enabled":true,"completionOptions":{"model":"gpt-4o","temperature":0.1}}]}