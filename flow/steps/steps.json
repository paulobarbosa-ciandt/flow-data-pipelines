{"dashboard":{"name":"[APAC Data Demo] E2E Delta Lake Deployment Acceleration","description":"End-to-end acceleration of Delta Lake deployments, from business requirements through sourcing and transforming data to quality and BI.","category":"507f1f77bcf86cd799439013","isActive":true,"columns":[{"name":"Business Requirements","description":"Business Requirements document to be parsed and converted into stories.","templates":["0"],"extensions":[]},{"name":"User Stories","description":"Stories broken down from requirements","templates":["1","2","3"],"extensions":[]},{"name":"Code","description":"Code to implement the data pipeline","templates":[],"extensions":[]}],"rules":[{"emails":[],"groups":[],"roles":["backlogrefiner.admin"]}]},"templates":[{"author":"4a8877b1fbb94571bb11d575","name":"data__business_requirements_to_stories","domain":"put_here_your_domain","category":"507f1f77bcf86cd799439011","context":"You are a Data Architect skilled in translating business requirements into clear, actionable agile user stories. Your task is to generate user stories and dbt models based solely on the provided requirements document, adhering strictly to the given templates and instructions. Integrate all business logic within the technical stories, and consolidate data quality requirements within the dbt schema files rather than as separate user stories. Use the following technology stack: **Azure** (cloud platform), Databricks (data processing and Delta Lake storage), and dbt (data transformation and data quality validations).\n\n**Important:**  \n\n- **Apply the naming convention directly in the templates.**  \n- **Only output the contents of each user story and dbt model, separated by the informed separator. No comments or extra information.**  \n- **Only the stories and models are needed. Do not output anything other than the stories and models.**  ","prompt":"A requirements document has been attached for implementing a data pipeline on our data platform.\n\n---\n\n#### **Your Tasks**\n\n1. **Extract and Validate Table Details:**  \n   - Parse the requirements document and **identify all tables** explicitly listed. For each table, extract:  \n     - Table Name  \n     - **Bronze Table Name**: `bronze_{{table_name}}`  \n     - **Silver Table Name**: `silver_{{table_name}}`  \n     - **Fields**: List of columns, data types, and constraints  \n     - **Source Details**: Location of the source table or files  \n     - Transformation logic, mappings, data quality checks, and dimensional modelling requirements.  \n   - **Validate Tables**: Ensure that only tables explicitly mentioned in the document are used to generate user stories and dbt models. Exclude any references to non-existing tables.\n\n2. **Iterate Over Valid Tables to Generate Stories and Models:**  \n   - **For each validated table identified in the document, generate the following:**  \n     - User stories for:  \n       - Data ingestion into the bronze layer.  \n       - Data transformation into the silver layer.  \n       - Dimensional modelling into the gold layer.  \n         - **Each table must have its own dimensional modelling story.**  \n     - Consolidate **data quality checks** into the dbt schema files rather than as separate user stories.  \n\n3. **Insert Separators Between Outputs:**  \n   - Place the separator `{{wf.template.splitBy}}` **only between** user stories and models.  \n   - Do not include the separator before the first output or after the last one.  \n   - Do not mention the separator within the outputs themselves.  \n\n4. **Output Format:**  \n   - **Only include** the content of each user story and dbt model followed by the separator.  \n   - **Exclude** any titles, comments, explanations, or additional text outside the outputs.  \n\n---\n\n#### **User Story Templates**\n\n##### **1. Ingest Source Data into Bronze Layer**\n\n- **As a** data engineer  \n- **I want** to ingest `{{table_name}}` from the source system into the bronze layer in Databricks on Azure  \n- **So that** it is available for downstream processing  \n\n- **Acceptance Criteria**  \n\n  - Data is ingested from `{{source_details}}` to `bronze.{{bronze_table_name}}` in Delta Lake format  \n  - Includes all relevant fields: `{{list_of_fields}}`  \n  - Handles data ingestion failures gracefully  \n\n- **Technical Details**  \n\n  - **Source Table**: `{{source_system}}.{{table_name}}`  \n  - **Target Table**: `bronze.{{bronze_table_name}}`  \n  - **Source Details**: `{{source_details}}`  \n  - **Destination Table Schema**  \n\n    ```\n    | Column Name   | Data Type     | Constraints        |\n    |---------------|---------------|--------------------|\n    | ...           | ...           | ...                |\n    ```  \n\n  - **Include Metadata Columns**  \n    - Add `create_date` and `update_date` columns.  \n  - **Ingestion Method**: `{{ingestion_method}}`  \n  - **Tools/Technologies**: Databricks on Azure, Delta Lake, Azure Blob Storage for storage  \n\n##### **2. Transform Data from Bronze to Silver Layer**\n\n- **As a** data engineer  \n- **I want** to transform and cleanse `{{table_name}}` from the bronze layer to the silver layer using dbt in Databricks on Azure, applying necessary business logic  \n- **So that** the data is accurate and ready for analytical processing  \n\n- **Acceptance Criteria**  \n\n  - Apply transformations to standardize data fields according to the data model  \n  - Integrate business logic calculations as specified  \n  - Ensure all transformations adhere to data governance and quality standards  \n\n- **Technical Details**  \n\n  - **Source Table**: `bronze.{{bronze_table_name}}`  \n  - **Target Table**: `silver.{{silver_table_name}}`  \n  - **Transformations and Business Logic**  \n    - **Calculations**: `{{calculations}}`  \n    - **Mappings**: `{{mappings}}`  \n      ```\n      | Source Value  | Mapped Value     |\n      |---------------|------------------|\n      | ...           | ...              |\n      ```  \n\n  - **Include Metadata Columns**  \n    - Add `create_date` and `update_date` columns.  \n  - **Tools/Technologies**: dbt, Databricks on Azure, Delta Lake  \n\n##### **3. Dimensional Modelling in Gold Layer (One Story Per Table)**\n\n- **As a** data engineer  \n- **I want** to design and implement the `{{table_name}}` dimensional model in the gold layer using Databricks and Delta Lake on Azure  \n- **So that** it supports efficient analytics and adheres to SCD requirements  \n\n- **Acceptance Criteria**  \n\n  - Create the `{{table_name}}` dimensional model with all specified attributes  \n  - Ensure SCD Type 2 implementation where applicable  \n  - Use snake_case naming conventions for all objects and columns  \n  - Add metadata columns such as `create_date`, `update_date`, and others based on best practices  \n\n- **Technical Details**  \n\n  - **Dimension Table Schema**  \n\n    ```\n    | Column Name   | Data Type     | Constraints        |\n    |---------------|---------------|--------------------|\n    | ...           | ...           | ...                |\n    ```  \n\n  - **Include Surrogate Keys** where applicable  \n  - **Tools/Technologies**: Databricks on Azure, Delta Lake  \n\n---\n\n**Separator Between Outputs**  \n\n```\n{{wf.template.splitBy}}\n```","example":"<Story Title>\nAS A <type of user>\nI WANT <brief description of the story>\nSO THAT <user's objective in up to 15 words>\n\nAcceptance Criteria\n<acceptance criteria>\n\nTechnical Details\n<technical details>","splitBy":"<<<->>>","enabled":true,"completionOptions":{"model":"gpt-4o","temperature":0.1}},{"author":"4a8877b1fbb94571bb11d575","name":"data__load_bronze","domain":"put_here_your_domain","category":"507f1f77bcf86cd799439013","context":"You are ChatGPT, a large language model trained by OpenAI with expertise in data engineering, dbt, Delta Lake, Python, and Azure Blob Storage within Databricks environments. Your role is to generate precise `dbt` model and schema files for ingesting Parquet data into the bronze layer of a data lake. Follow best practices, use **snake_case** for all identifiers, and ensure consistency with the provided source configurations.","prompt":"{{wf.parent.content}}\n\n---\n\n### **Data Source Configuration**\n\n- **Azure Blob Storage Details:**\n  - **Storage Account**: `flowdataefficiencydemo`\n  - **Container Name**: `fivetran`\n  - **File Format**: Parquet\n  - **File Location Pattern**: Data for each source table is organized into separate directories within the `fivetran` container following the naming pattern `raw_<source_table_name>`.\n    - *Example*: Data from the source table `members` is located in `fivetran/raw_members`.\n  - **Destination Table**: `bronze.<source_table_name>`\n\n---\n\n### **Requirements**\n\n#### **1. Data Ingestion**\n\n- **Assumption**: Fivetran is already configured to extract data from the SQL Server database and load it into the Azure Blob Storage container named `fivetran` under the account `flowdataefficiencydemo`.\n\n#### **2. dbt Model and Schema File Generation**\n\n- **Model File Naming:**\n  - **Naming Convention**: The `dbt` model file should be named **exactly** as the table name with a `.sql` extension.\n    - *Example*: For the table `members`, the model file should be `members.sql`.\n\n- **Schema File Generation:**\n  - **Mandatory Schema Files**: For each `dbt` model, generate a corresponding schema file named `<table_name>.yml` (e.g., `members.yml`).\n  - **Schema Configuration**: The schema file should include:\n    - **Models**: Metadata and descriptions for each column.\n    - **Exclusions**: **Do not** include any constraints, data quality tests, or validation rules.\n\n- **Delta Table Creation:**\n  - Generate `dbt` models to create the Delta table in the bronze layer if it does not exist.\n  - If the Delta table exists and there are schema changes, generate `dbt` models to alter the table accordingly.\n  - **Schema Adherence**: Ensure that the `dbt` models strictly adhere to the source table schemas as defined in the Hive Metastore catalog, maintaining a 1-to-1 mapping without introducing additional columns.\n\n- **Partitioning:**\n  - Suggest appropriate partition keys based on the source schema to optimize performance.\n\n- **Delta Table Settings:**\n  - Include relevant Delta table configurations to enhance performance and reliability.\n\n- **Metadata Columns:**\n  - Add `create_date` and `update_date` columns to the Delta table using appropriate data types (e.g., `TIMESTAMP`).\n\n- **Error Handling:**\n  - Ensure that the `dbt` models handle data ingestion failures gracefully.\n  - Incorporate necessary logging mechanisms.\n\n- **Best Practices:**\n  - Follow best practices for Delta Lake, `dbt`, and Python development.\n  - Use **snake_case** for all data objects and column names.\n  - Avoid hardcoding sensitive information; utilize secure parameter storage where necessary.\n\n- **Specific Adjustments:**\n  - **Incremental Strategy**: Use the `merge` strategy without including aggregation functions in the `WHERE` clause, as Databricks does not handle them efficiently. Do not include an `is_incremental()` block if there are no additional conditions or transformations needed for incremental loads.\n  - **Source Reference**: Ensure that the `{{ source('fivetran', 'raw_<source_table_name>') }}` reference is properly configured.\n\n#### **3. Source Configuration File (`sources.yml`)**\n\n- **Generation/Update Instructions:**\n  - Generate or update a `sources.yml` file within a directory named `sources` in the project root.\n  - The `sources.yml` file should define the source configurations as follows:\n\n```yaml\nversion: 2\n\nsources:\n  - name: fivetran\n    schema: fivetran  \n    tables:\n      - name: raw_<source_table_name>\n```\n\n- **Ensure Consistency**: The source configurations in `sources.yml` should align with the `dbt` models to maintain consistency and integrity.\n\n#### **4. Schema File (`<table_name>.yml`)**\n\n- **Generation/Update Instructions:**\n  - For each `dbt` model, generate or update a corresponding schema file named `<table_name>.yml` (e.g., `members.yml`).\n  - The schema file should include:\n    - **Model Description**: A brief description of the model's purpose.\n    - **Columns**: Definitions for each column, including descriptions and data types.\n    - **Exclusions**: **Do not** include any constraints, data quality tests, or validation rules.\n\n- **Example Schema Configuration:**\n\n```yaml\nversion: 2\n\nmodels:\n  - name: members\n    description: \"Bronze layer table for members data ingested from Azure Blob Storage.\"\n    columns:\n      - name: member_id\n        description: \"Unique identifier for each member.\"\n      - name: first_name\n        description: \"Member's first name.\"\n      - name: last_name\n        description: \"Member's last name.\"\n      - name: date_of_birth\n        description: \"Member's date of birth.\"\n      - name: gender\n        description: \"Member's gender.\"\n      - name: address\n        description: \"Member's address.\"\n      - name: phone_number\n        description: \"Member's phone number.\"\n      - name: email\n        description: \"Member's email address.\"\n      - name: join_date\n        description: \"Date when the member joined.\"\n      - name: employer_id\n        description: \"Identifier for the member's employer.\"\n      - name: modified_date\n        description: \"Timestamp of the last modification.\"\n      - name: create_date\n        description: \"Timestamp when the record was created.\"\n      - name: update_date\n        description: \"Timestamp when the record was last updated.\"\n```\n\n#### **5. Additional Recommendations**\n\n- Suggest any improvements or optimizations for the data ingestion process to enhance efficiency, reliability, or scalability.\n\n---\n\n### **Example Adjustments Based on Existing Issues**\n\n- **Removing Aggregation in WHERE Clause**: Do not include date checks using aggregation functions in the `WHERE` clause. Rely solely on the `merge` strategy for incremental updates.\n  \n- **Excluding Empty `is_incremental()` Blocks**: Omit the `is_incremental()` conditional block if there are no additional conditions or transformations required for incremental loads.\n  \n- **Including `sources.yml` Configuration**: Ensure that the `sources.yml` file is generated or updated to define the source tables referenced in the `dbt` models.\n\n- **Generating Schema Files Without Constraints or Tests**: For each `dbt` model, generate a corresponding schema file (`<table_name>.yml`) that includes only model descriptions and column metadata, excluding any constraints or data quality tests.\n\n---\n\n### **Expected Output**\n\n1. **dbt Model File**: A properly configured `dbt` model for ingesting data into the bronze layer without using aggregation functions in the `WHERE` clause and without unnecessary `is_incremental()` blocks. The model file should be named after the table (e.g., `members.sql`).\n\n2. **Schema File**: A corresponding schema file (`members.yml`) located alongside the model file with accurate model descriptions and column definitions **without** any constraints or data quality tests.\n\n3. **sources.yml File**: A `sources.yml` file located in the `sources` directory with accurate source configurations.\n\n---\n\n### **Sample dbt Model Generated**\n\n```sql\n{{ config(\n    materialized='incremental',\n    unique_key='member_id',\n    incremental_strategy='merge',\n    on_schema_change='sync_all_columns'\n) }}\n\nWITH source_data AS (\n    SELECT\n        member_id,\n        first_name,\n        last_name,\n        date_of_birth,\n        gender,\n        address,\n        phone_number,\n        email,\n        join_date,\n        employer_id,\n        modified_date,\n        current_timestamp() AS create_date,\n        current_timestamp() AS update_date\n    FROM\n        {{ source('fivetran', 'raw_members') }}\n)\n\nSELECT\n    member_id,\n    first_name,\n    last_name,\n    date_of_birth,\n    gender,\n    address,\n    phone_number,\n    email,\n    join_date,\n    employer_id,\n    modified_date,\n    create_date,\n    update_date\nFROM\n    source_data\n```\n\n### **Sample Schema File (`members.yml`)**\n\n```yaml\nversion: 2\n\nmodels:\n  - name: members\n    description: \"Bronze layer table for members data ingested from Azure Blob Storage.\"\n    columns:\n      - name: member_id\n        description: \"Unique identifier for each member.\"\n      - name: first_name\n        description: \"Member's first name.\"\n      - name: last_name\n        description: \"Member's last name.\"\n      - name: date_of_birth\n        description: \"Member's date of birth.\"\n      - name: gender\n        description: \"Member's gender.\"\n      - name: address\n        description: \"Member's address.\"\n      - name: phone_number\n        description: \"Member's phone number.\"\n      - name: email\n        description: \"Member's email address.\"\n      - name: join_date\n        description: \"Date when the member joined.\"\n      - name: employer_id\n        description: \"Identifier for the member's employer.\"\n      - name: modified_date\n        description: \"Timestamp of the last modification.\"\n      - name: create_date\n        description: \"Timestamp when the record was created.\"\n      - name: update_date\n        description: \"Timestamp when the record was last updated.\"\n```\n\n### **Sample sources.yml File**\n\n```yaml\nversion: 2\n\nsources:\n  - name: fivetran\n    schema: fivetran  \n    tables:\n      - name: raw_members\n```","example":"","splitBy":"","enabled":true,"completionOptions":{"model":"gpt-4o","temperature":0.1}},{"author":"4a8877b1fbb94571bb11d575","name":"data__load_silver","domain":"put_here_your_domain","category":"507f1f77bcf86cd799439013","context":"You are ChatGPT, a large language model trained by OpenAI with expertise in data engineering, dbt, Delta Lake, Python, and Azure Databricks environments. Your role is to generate precise dbt model and schema files for transforming and cleansing data from the bronze layer to the silver layer of a data lake, adhering to best practices, the `<layer>_<table_name>` naming convention, and the provided technical specifications.","prompt":"{{wf.parent.content}}\n\n---\n\n**Source Details: Bronze Layer**\n\n- **Primary Source Table**: {{primary_source_table_name}}\n- **Additional Source Tables**: \n  {% if additional_source_tables %}\n    {% for table in additional_source_tables %}\n      - **Table Name**: {{ table.name }}\n        - **Schema**: bronze\n        - **Data Format**: Delta Lake\n        - **Storage Location**: Delta tables in Databricks on Azure\n    {% endfor %}\n  {% endif %}\n\n- **Schema**: bronze\n- **Data Format**: Delta Lake\n- **Storage Location**: Delta tables in Databricks on Azure\n\n**Target Details: Silver Layer**\n\n- **Target Table**: {{target_table_name}}\n- **Schema**: silver\n- **Data Format**: Delta Lake\n- **Storage Location**: Delta tables in Databricks on Azure\n\n---\n\n**Transformations and Business Logic:**\n\n{% if transformations %}\n### Calculations:\n{% for calc in transformations.calculations %}\n- `{{ calc.expression }}` as `{{ calc.alias }}`\n{% endfor %}\n\n### Mappings:\n- **Standardize String Fields to Proper Case**\n  ```\n  | Source Column | Transformation          | Target Column     |\n  |---------------|-------------------------|-------------------|\n  {% for mapping in transformations.mappings %}\n  | {{ mapping.source }} | {{ mapping.transformation }} | {{ mapping.target }} |\n  {% endfor %}\n  ```\n{% endif %}\n\n### Data Quality Checks:\n- **Key Columns Should Not Contain Nulls:** `{{ key_columns | join(', ') }}`\n- **Categorical Fields Should Contain Only Expected Values:** \n  - `{{ categorical_field }} IN ({{ expected_values | join(', ') }})`\n\n---\n\n**Requirements:**\n\n- **dbt Model Generation:**\n  - Create a dbt model to transform data from `bronze.{{primary_source_table_name}}` and any **additional source tables** (e.g., `bronze.{{table.name}}`) to `silver.{{target_table_name}}`.\n  - Ensure transformations adhere strictly to the specified business logic and mappings.\n  - Maintain a 1-to-1 mapping of necessary columns without introducing additional columns unless specified.\n  - Appropriately join additional source tables based on the transformation requirements.\n  - **Adhere to the `<layer>_<table_name>` naming convention for all table references.** Use the provided table names as is.\n\n- **Schema Configuration (`schema.yml`):**\n  - Define the target table schema as specified in the technical details.\n  - Include `create_date` and `update_date` metadata columns with appropriate data types (e.g., TIMESTAMP).\n  - **Integrate Data Quality Checks as dbt Tests:**\n    - Add `not_null`, `unique`, and `accepted_values` tests based on the specified data quality checks.\n  \n- **Incremental Loading:**\n  - Implement incremental models using the `merge` strategy without additional WHERE clauses that filter based on dates.\n  - Rely on the merge strategy to handle data updates efficiently.\n\n- **Source Configuration (`sources.yml`):**\n  - Ensure that the `sources.yml` file includes the `silver` schema and references to `bronze.{{primary_source_table_name}}` as well as any **additional source tables**.\n  - If `sources.yml` does not exist or needs updating, generate or modify it accordingly.\n  - **Maintain the `<layer>_<table_name>` naming convention within the `sources.yml` file.**\n\n- **Target Schema Configuration:**\n  - Generate a `schemas/{{target_table_name}}.yml` file within a directory named `schemas` in the project root.\n  - Define the target table's schema, including columns, data types, and descriptions as specified.\n  - **Do not include any constraints** since Databricks does not enforce them.\n  - **Do not include any quality tests** in the schema files beyond dbt's standard tests.\n  - Ensure all table and column names follow the `<layer>_<table_name>` naming convention.\n\n- **Error Handling:**\n  - Implement graceful error handling within the dbt models.\n  - Include logging mechanisms to capture and report data transformation issues.\n\n- **Best Practices:**\n  - Use **snake_case** for all data objects and column names.\n  - Avoid hardcoding sensitive information; utilize secure parameter storage where necessary.\n  - Follow best practices for Delta Lake and dbt development to ensure performance and maintainability.\n  - **Ensure all table names adhere to the `<layer>_<table_name>` naming convention.** Use the provided table names as is in all `source` function references.\n\n- **Additional Recommendations:**\n  - Suggest optimizations for the transformation process, such as indexing frequently queried columns.\n  - Recommend any additional metadata or documentation that could enhance data quality and usability.\n\n---\n\n**Technical Details:**\n\n- **Tools/Technologies**: dbt, Databricks on Azure, Delta Lake\n- **Programming Language**: SQL with Jinja templating for dbt\n\n---\n\n**Example Generated Files:**\n\n1. **dbt Model (`silver_members.sql`):**\n\n    ```sql\n    {{ config(\n        materialized='incremental',\n        unique_key='member_id',\n        incremental_strategy='merge',\n        on_schema_change='sync_all_columns'\n    ) }}\n\n    WITH source_data AS (\n        SELECT\n            m.member_id,\n            CONCAT(UPPER(SUBSTRING(m.first_name, 1, 1)), LOWER(SUBSTRING(m.first_name, 2))) AS first_name,\n            CONCAT(UPPER(SUBSTRING(m.last_name, 1, 1)), LOWER(SUBSTRING(m.last_name, 2))) AS last_name,\n            m.date_of_birth,\n            m.gender,\n            m.join_date,\n            m.employer_id,\n            SUM(c.amount) - SUM(w.amount) AS account_balance,\n            current_timestamp() AS create_date,\n            current_timestamp() AS update_date\n        FROM\n            {{ source('bronze', '{{primary_source_table_name}}') }} AS m\n            LEFT JOIN {{ source('bronze', 'bronze_contributions') }} AS c ON m.member_id = c.member_id\n            LEFT JOIN {{ source('bronze', 'bronze_withdrawals') }} AS w ON m.member_id = w.member_id\n        GROUP BY\n            m.member_id, m.first_name, m.last_name, m.date_of_birth, m.gender, m.join_date, m.employer_id\n    )\n\n    SELECT\n        member_id,\n        CONCAT(first_name, ' ', last_name) AS full_name,\n        date_of_birth,\n        gender,\n        join_date,\n        employer_id,\n        account_balance,\n        create_date,\n        update_date\n    FROM\n        source_data\n    ```\n\n2. **Sources Configuration (`sources.yml`):**\n\n    ```yaml\n    version: 2\n\n    sources:\n      - name: bronze\n        schema: bronze\n        tables:\n          - name: bronze_members\n          - name: bronze_contributions\n          - name: bronze_withdrawals\n\n      - name: silver\n        schema: silver\n        tables:\n          - name: silver_members\n    ```\n\n3. **Target Table Schema (`schemas/silver_members.yml`):**\n\n    ```yaml\n    version: 2\n\n    models:\n      - name: silver_members\n        description: \"Transformed and cleansed data from bronze.bronze_members, bronze.bronze_contributions, and bronze.bronze_withdrawals for analytical processing.\"\n        columns:\n          - name: member_id\n            description: \"Unique identifier for each member.\"\n            data_type: INT\n            tests:\n              - not_null\n              - unique\n          - name: first_name\n            description: \"Member's first name, standardized to proper case.\"\n            data_type: VARCHAR(50)\n            tests:\n              - not_null\n          - name: last_name\n            description: \"Member's last name, standardized to proper case.\"\n            data_type: VARCHAR(50)\n            tests:\n              - not_null\n          - name: date_of_birth\n            description: \"Member's date of birth.\"\n            data_type: DATE\n            tests:\n              - not_null\n          - name: gender\n            description: \"Member's gender.\"\n            data_type: CHAR(1)\n            tests:\n              - not_null\n              - accepted_values:\n                  values: ['M', 'F', 'O']  # Example accepted values\n          - name: join_date\n            description: \"Date when the member joined.\"\n            data_type: DATE\n            tests:\n              - not_null\n          - name: employer_id\n            description: \"Identifier for the member's employer.\"\n            data_type: INT\n            tests:\n              - not_null\n          - name: account_balance\n            description: \"Calculated account balance for the member.\"\n            data_type: DECIMAL(10,2)\n            tests:\n              - not_null\n          - name: create_date\n            description: \"Timestamp when the record was created.\"\n            data_type: TIMESTAMP\n            tests:\n              - not_null\n          - name: update_date\n            description: \"Timestamp when the record was last updated.\"\n            data_type: TIMESTAMP\n            tests:\n              - not_null\n        meta:\n          created_at: \"2024-04-27T12:00:00Z\"\n          updated_at: \"2024-04-27T12:00:00Z\"\n    ```\n\n---\n\n### **Detailed Explanation of Adjustments:**\n\n1. **Incorporate Layer Prefixes Directly in Table Names:**\n   - **User Input:** By requiring users to provide table names with the layer prefix (e.g., `bronze_members`), we eliminate the need for the prompt to add them automatically, ensuring clarity and consistency.\n   - **Prompt Adjustment:** Removed any logic that attempts to prepend layer prefixes. The prompt now uses the provided table names directly in all references.\n\n2. **Integrate Data Quality Checks as dbt Tests:**\n   - **Schema Configuration (`schema.yml`):** Added a `tests` section under each column in the `schema.yml` file. These tests leverage dbt's built-in testing framework to enforce data quality constraints.\n     - **Example Tests:**\n       - `not_null`: Ensures that the column does not contain null values.\n       - `unique`: Ensures that all values in the column are unique.\n       - `accepted_values`: Ensures that the column contains only specified values (useful for categorical fields).\n\n3. **Consistent Naming Conventions Across All Files:**\n   - **dbt Models, `schema.yml`, and `sources.yml`:** All references to tables now include the layer prefix, maintaining consistency and preventing any naming discrepancies.\n\n4. **Explicit Instructions for Data Quality Checks:**\n   - **Transformations and Business Logic Section:** Clearly defines what data quality checks need to be implemented, which the prompt translates into dbt tests within the `schema.yml` files.\n\n5. **Eliminate Ambiguity in Join Conditions:**\n   - **Join Conditions:** Since table names include layer prefixes, join conditions are straightforward and unambiguous, referencing fully prefixed table names.\n\n---\n\n### **Updated User Prompt Example:**\n\nTo ensure the prompt functions as intended, here's how your **User Prompt** should look with fully prefixed table names:\n\n```markdown\n- **As a** data engineer\n- **I want** to transform and cleanse `bronze_members`, `bronze_contributions`, and `bronze_withdrawals` from the bronze layer to the silver layer using dbt in Databricks on Azure, applying necessary business logic\n- **So that** the data is accurate and ready for analytical processing\n\n- **Acceptance Criteria**\n\n  - Apply transformations to standardize data fields according to the data model\n  - Integrate business logic calculations as specified\n  - Ensure data passes all quality checks\n\n- **Technical Details**\n\n  - **Source Tables**: `bronze_members`, `bronze_contributions`, `bronze_withdrawals`\n  - **Target Table**: `silver_members`\n  - **Transformations and Business Logic**\n    - **Calculations**: `account_balance = SUM(bronze_contributions.amount) - SUM(bronze_withdrawals.amount)`\n    - **Mappings**: `Standardize string fields to proper case`\n      ```\n      | Source Column | Transformation          | Target Column     |\n      |---------------|-------------------------|-------------------|\n      | first_name    | Proper Case             | first_name        |\n      | last_name     | Proper Case             | last_name         |\n      ```\n  - **Data Types**\n    ```\n    | Column Name      | Data Type     |\n    |------------------|---------------|\n    | member_id        | INT           |\n    | full_name        | VARCHAR(100)  |\n    | date_of_birth    | DATE          |\n    | gender           | CHAR(1)       |\n    | join_date        | DATE          |\n    | employer_id      | INT           |\n    | account_balance  | DECIMAL(10,2) |\n    | create_date      | TIMESTAMP     |\n    | update_date      | TIMESTAMP     |\n    ```\n\n  - **Include Metadata Columns**\n    - Add `create_date` and `update_date` columns.\n  - **Ingestion Method**: Incremental load using `merge` strategy\n  - **Tools/Technologies**: dbt, Databricks on Azure, Delta Lake\n\n  - **Join Conditions:**\n    - `bronze_members.member_id = bronze_contributions.member_id`\n    - `bronze_members.member_id = bronze_withdrawals.member_id`","example":"","splitBy":"","enabled":true,"completionOptions":{"model":"gpt-4o","temperature":0.1}},{"author":"4a8877b1fbb94571bb11d575","name":"data__load_gold","domain":"put_here_your_domain","category":"507f1f77bcf86cd799439013","context":"You are ChatGPT, an expert in **data engineering, dbt, dimensional modelling**, and **modern data architectures**. Your role is to design and generate precise **dbt model, schema, and snapshot files** for implementing a **dimensional or fact table** in the **gold layer** of a Delta Lake within a Databricks environment on Azure. You must use **dbt snapshots** to manage Slowly Changing Dimensions (SCD2) for dimension tables, while ensuring adherence to **dbt conventions**, table naming requirements, and best practices.","prompt":"{{wf.parent.content}}  \n\n---\n\n### **Requirements**\n\n#### **1. Model Type Identification**\n- Based on the user story:\n  - **Dimension Table**:\n    - Table names must start with `dim_` (e.g., `dim_members`).\n    - Leverage **dbt snapshots** to implement SCD Type 2 handling.\n    - Ensure the snapshot file captures changes using the provided structure, with the `strategy` set to `timestamp` and `updated_at` referencing the appropriate timestamp column.\n    - Add metadata columns such as `member_sk` (surrogate key) and standard SCD2 attributes like `effective_date` and `expiry_date` in the final model.\n  - **Fact Table**:\n    - Table names must start with `fact_` (e.g., `fact_sales`).\n    - Implement required aggregations or calculations based on the source data.\n    - Ensure the table supports query performance with relevant indexing or partitioning strategies.\n    - Add metadata columns such as `create_date` and `update_date`.\n\n#### **2. dbt Snapshot File Creation**\n- **File Naming**:\n  - Generate a snapshot file for each dimension table named `snapshot_<silver_source_table_name>.sql` (e.g., `snapshot_silver_members.sql`).\n- **Snapshot Logic**:\n  - Use the `timestamp` strategy to track changes.\n  - Reference the source table in the silver layer using `{{ source() }}`.\n  - Generate a surrogate key (`member_sk` or equivalent) using `dbt_utils.surrogate_key()` and a list of key columns specified in the user story or inferred from the schema.\n  - Configure the snapshot to handle hard deletes using `invalidate_hard_deletes = True`.\n\n#### **3. dbt Model File Creation**\n- **File Naming**:\n  - For dimensions: Generate a file named `dim_<table_name>.sql` (e.g., `dim_members.sql`).\n  - For facts: Generate a file named `fact_<table_name>.sql` (e.g., `fact_sales.sql`).\n- **Table Creation**:\n  - Use dbt to create the final table in the gold layer, following the provided table name and prefix.\n  - Reference the snapshot output for dimension tables (`{{ ref('snapshot_<silver_source_table_name>') }}`).\n  - Implement **incremental loading** using the `merge` strategy for both fact and dimension tables.\n\n#### **4. Schema File Configuration**\n- **File Naming**:\n  - For dimensions: Generate a schema file named `dim_<table_name>.yml` (e.g., `dim_members.yml`).\n  - For facts: Generate a schema file named `fact_<table_name>.yml` (e.g., `fact_sales.yml`).\n- **Content**:\n  - Define the table structure, including columns, descriptions, and data types.\n  - Add **dbt tests** for key columns (`not_null`, `unique`, `accepted_values` as applicable).\n\n#### **5. Incremental Loading**\n- Use the `merge` strategy in dbt models to handle incremental updates:\n  - Insert new records.\n  - Update existing records based on the snapshot data.\n\n#### **6. Optimisations**\n- Use Delta Lake features to improve performance:\n  - Partitioning (e.g., by `effective_date` for dimensions or `transaction_date` for facts).\n  - Z-Ordering on frequently queried columns.\n- Avoid hardcoding values; use parameters or Jinja templating.\n\n#### **7. Documentation**\n- Include in-line comments in the dbt model and snapshot files to explain key logic and transformations.\n- Provide a brief description of the table and snapshot purpose in the schema file.\n\n---\n\n### **Expected Output**\n\n1. **dbt Model File**:  \n   - A dbt model in SQL, following the specified table naming convention (e.g., `dim_<table_name>.sql` or `fact_<table_name>.sql`), implementing the required transformations, joins, and incremental logic.  \n2. **Schema File**:  \n   - A schema file describing the table structure, including metadata, data types, constraints, and tests (e.g., `dim_<table_name>.yml` or `fact_<table_name>.yml`).  \n3. **Snapshot File**:  \n   - A dbt snapshot file named `snapshot_<silver_source_table_name>.sql` for dimension tables, implementing the SCD2 logic.  \n4. **Additional Recommendations**:  \n   - Suggested optimisations for performance, scalability, or maintainability based on the user story.  \n\n---\n\n### **Example Outputs**\n\n#### **For a Dimension Table**\n\n**dbt Snapshot (`snapshot_silver_members.sql`)**:\n```sql\n{% snapshot snapshot_silver_members %}\n\n{{\n  config(\n    target_schema = 'silver',\n    unique_key = 'member_id',\n    strategy = 'timestamp',\n    updated_at='update_date',\n    invalidate_hard_deletes = True\n  )\n}}\n\n{%- set key_cols = ['member_id', 'date_of_birth', 'create_date'] -%}\n\nSELECT\n    {{ dbt_utils.surrogate_key(key_cols) }} AS member_sk,\n    *\nFROM {{ source('silver', 'silver_members') }}\n\n{% endsnapshot %}\n```\n\n**dbt Model (`dim_members.sql`)**:\n```sql\n{{ config(\n    materialized='incremental',\n    unique_key='member_sk',\n    incremental_strategy='merge',\n    on_schema_change='sync_all_columns'\n) }}\n\nWITH snapshot_data AS (\n    SELECT\n        member_sk,\n        member_id,\n        full_name,\n        date_of_birth,\n        gender,\n        join_date,\n        employer_id,\n        create_date,\n        update_date,\n        is_active,\n        effective_date,\n        expiry_date\n    FROM {{ ref('snapshot_silver_members') }}\n)\n\nSELECT\n    member_sk,\n    member_id,\n    full_name,\n    date_of_birth,\n    gender,\n    join_date,\n    employer_id,\n    create_date,\n    update_date,\n    is_active,\n    effective_date,\n    expiry_date\nFROM snapshot_data\n```\n\n**Schema File (`dim_members.yml`)**:\n```yaml\nversion: 2\n\nmodels:\n  - name: dim_members\n    description: \"Dimension table for member information in the gold layer, tracking historical changes using SCD Type 2.\"\n    columns:\n      - name: member_sk\n        description: \"Surrogate key for the member dimension.\"\n        data_type: STRING\n      - name: member_id\n        description: \"Unique identifier for each member.\"\n        data_type: INT\n      - name: effective_date\n        description: \"Date when the record became effective.\"\n        data_type: DATE\n      - name: expiry_date\n        description: \"Date when the record expired.\"\n        data_type: DATE\n      - name: is_active\n        description: \"Indicates whether the record is active.\"\n        data_type: BOOLEAN\n```","example":"","splitBy":"","enabled":true,"completionOptions":{"model":"gpt-4o","temperature":0.1}}]}